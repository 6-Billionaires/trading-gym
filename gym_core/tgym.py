import logging
from gym.core import Env
import pandas as pd
import numpy as np
import glob
from util.Exception import TradingException
import random
import datetime
import math
import config

C_HOME_FULL_DIR = config.GYM['HOME']


class TradingGymEnv(Env):

    d_episodes_data = {}

    # every agent has their own constraints to be managed by trading gym itself.
    c_agent_max_num_of_allowed_transaction = 10

    # c_agent_prev_step_start_yyyymmdd_in_episode = '090501'  # 9hr 6min 1sec
    # c_agent_step_start_yyyymmdd_in_episode = '090601' # 9hr 6min 1sec

    c_agent_prev_step_start_datetime_in_episode = None
    c_agent_step_start_datetime_in_episode = None

    c_agent_range_timestamp = []

    p_agent_current_num_transaction = 0
    p_agent_current_episode_ref_idx = 0
    p_agent_current_episode_data_quote = None
    p_agent_current_episode_data_order = None
    p_agent_current_episode_ticker = None
    p_agent_current_episode_date = None
    p_agent_current_episode_price_history = []
    p_agent_current_step_in_episode = 0 # step interval = 1sec

    p_agent_is_stop_loss = None
    p_agent_is_reached_goal = None


    is_data_loaded = False
    episode_data_count = 0

    """
    This class's super class is from OpenAI Gym and extends to provide trading environment

    # reference
        - keyword for trading
            . http://www.fo24.co.kr/bbs/print_view.html?id=167497&code=bbs203
        - doc
            . http://docs.python-guide.org/en/latest/writing/documentation/
        - style guide to write comment ( or docstring)
            . https://medium.com/@kkweon/%ED%8C%8C%EC%9D%B4%EC%8D%AC-doc-%EC%8A%A4%ED%83%80%EC%9D%BC-%EA%B0%80%EC%9D%B4%EB%93%9C%EC%97%90-%EB%8C%80%ED%95%9C-%EC%A0%95%EB%A6%AC-b6d27cd0a27c
            . http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html
    """
    def _is_done(self):
        """
        check whether or not this episode is ended.

        Environment side
        ========================
        . current step exceeds in max_steps
        . exceed episode time bound

        Agent side
        ========================
        . lose all money agent has (= no balance)
        . exceed the count of available transaction

        :return: True or False

        """
        # TODO : here we need to decide whether or not episode ends right way or wait until end of duration of original duration
        if self.p_agent_current_num_transaction >= self.p_agent_max_num_of_allowed_transaction:
            return True
        elif self._current_agent_time_stamp > self.episode_duration_min:
            return True
        elif self.p_agent_is_stop_loss:
            return True
        else:
            return False

    def create_episode_data(self, episode_type):
        """
            it is executed just one time right after this gym is made for the first time.

            # 1. Loading data into gym
                It loads csv files based on its episode type. Before, we need to ensure that those csv files have to locate in a
                directory where trading-episode-filter gathered according to rules of episode type

            # 2. the file to be loaded
                - data/episode_type/ticker/episode_type-ticker-yyyymmdd.csv

            # 3. csv format is look like below
                episode_type-AAPL-yyyymmdd-quote.csv,
                episode_type-AAPL-yyyymmdd-order.csv
        """
        for item in glob.glob(C_HOME_FULL_DIR + '/data/' + episode_type + '/*'):
            for pf in glob.glob(item + '/*.csv', ):
                f = pf.split('\\')[-1]
                """
                1. condition
                    for now, there is only one episode type. 
                     - "episode_type" : 0

                2. data shape
                    {
                        "episode_idx" : "episode_data" : {
                                                                "meta" : {
                                                                    "ticker" : "AAPL",
                                                                    "date"   : "20180501"
                                                                },
                                                                "quote" : dataframe generated by pandas after reding csv, 
                                                                "order" : dataframe generated by pandas after reding csv,
                                                                }
                                                         }
                """

                current_ticker = f.split('-')[1]
                current_date = f.split('-')[2]

                d_meta = {'ticker': current_ticker, "date": current_date}  # 1

                # TODO : it needs to be update load dataset out of file into pandas dataframe
                if f.endswith('-order.csv'):
                    d_order = pd.read_csv(f)  # 2
                elif f.endswith('-quotes.csv'):
                    d_quote = pd.read_csv(f)  # 3
                else:
                    raise TradingException('it found out a file followed by wrong convention.')

                # TODO : delete below. it is just fake data of two above
                d_order = pd.DataFrame(np.random.randn(3600, 20),
                                       columns=['ask_price1', 'ask_price2', 'ask_price3', 'ask_price4', 'ask_price5',
                                                'ask_price6', 'ask_price7', 'ask_price8', 'ask_price9', 'ask_price10',
                                                'bid_price1', 'bid_price2', 'bid_price3', 'bid_price4', 'bid_price5',
                                                'bid_price6', 'bid_price7', 'bid_price8', 'bid_price9', 'bid_price10',
                                                ],
                                       index=pd.date_range(start='1/1/2016', periods=60 * 60, freq='S'))

                d_quote = pd.DataFrame(np.random.randn(3600, 11),
                                       columns=['buy_amount', 'buy_weighted_price', 'sell_amount',
                                                'sell_weighted_price', 'total_amount', 'total_weighted_price',
                                                'executed_strength', 'open', 'high', 'low', 'present_price'],
                                       index=pd.date_range(start='1/1/2016', periods=60 * 60, freq='S'))

                d_episode_data = {}
                d_episode_data['meta'] = d_meta
                d_episode_data['quote'] = d_quote
                d_episode_data['order'] = d_order

                self.d_episodes_data[self.episode_idx] = d_episode_data

            return self.episode_idx

    def __init__(self, episode_type=None, episode_duration_min = 60, step_interval='1s', percent_stop_loss=None, percent_goal_profit = 2,
                 balanace = None, max_num_of_transaction=None, obs_transform=None):
        """
        Initialize environment

        Args:
            episode type
                0 : treat equity which reached upper limit yesterday ( in development )
                1 : treat equity which start 5% higher price compared to close price yesterday ( not development yet )
            episode_duration_min
                this parameter will limit episode time
            step_interval
                this value is consist of number and unit of time ('s' for second, 'm' for minute, 'h' for hour ,'d' for day)
            percent_stop_loss
                positive. percentage, if a equity which agent holds fall down down to its percentage, it will be sold automatically.
            percent_goal_profit
                positive. percentage, if action from step results in this profit within duration, this action is considered as good and agent would get good reward from this action.
        """

        if not self.is_data_loaded:
            self.episode_data_count = self.create_episode_data(episode_type)

        # this parameter is belong to episode type so that it isn't necessary anymore.
        self.episode_duration_min = 60
        self.percent_stop_loss = percent_stop_loss
        self.percent_goal_profit = percent_goal_profit
        self.n_actions = None
        self.state_shape = None
        self.interval = 1   # 1 second
        self.ob_transform = obs_transform
        self.c_agent_range_timestamp = pd.date_range(
            self.c_agent_step_start_datetime_in_episode, periods=60*60, freq='S')
        self.p_agent_current_step_in_episode = 0

        # for now, episode type is not considered.
        self.p_agent_current_episode_ref_idx = random.randint(0, self.episode_data_count)
        self.p_agent_current_step_in_episode = 0

        return self._get_observation()

    def _rewards(self):
        """
        for now, reward is just additional information other than observation itself.
        @return: the price of the current episode's equity's price 60 secs ahead
        """

    def _get_observation(self):
        """
        Obesrvation information
        ===============================
        0. time : yyyymmdd
        1. bid / ask quote top 10
        2. open high close low based on step_interval in env
        3. price history for 1 minute
        """
        p0 = self.d_episodes_data[self.p_agent_current_episode_ref_idx]['quote'][self.c_agent_range_timestamp[self.p_agent_current_step_in_episode]]
        p1 =  self.d_episodes_data[self.p_agent_current_episode_ref_idx]['order'][self.c_agent_range_timestamp[self.p_agent_current_step_in_episode]]
        p2 = self.p_agent_current_episode_price_history
        return p0, p1, p2

    def step(self, action):
        """
        Here is the interface to be called by its agent.
        _get_observation needs to be transformed using transform observation that __init__ received.

        Args:
            action (object): for now, action is simple! 1 or -1 ( buy all or sell all )
        Returns:
            observation (object): agent's observation of the current environment
            reward (float) : amount of reward returned after previous action
            done (boolean): whether the episode has ended, in which case further step() calls will return undefined results
            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)
        """
        self.p_agent_current_step_in_episode = self.p_agent_current_step_in_episode + 1

        base_price = self.p_agent_current_episode_data_quote['price']
        info = []

        best_price = -1
        self.p_agent_is_stop_loss = False
        self.p_agent_is_reached_goal = False

        for present_ts in range(self.c_agent_range_timestamp[self.p_agent_current_step_in_episode],
                                self.c_agent_range_timestamp[self.p_agent_current_step_in_episode+60]):

            present_price = self.p_agent_current_episode_data_quote[present_ts]['ASK1']
            percent = (present_price - base_price) / base_price * 100
            if not self.p_agent_is_reached_goal and percent < 0 and self.percent_stop_loss <= np.abs(percent):
                self.p_agent_is_stop_loss = True
                self.p_agent_is_stop_loss_price = present_price #TODO: ASK1 is correct price for stop loss ?!
                break
            elif percent > 0 and self.percent_goal_profit >= np.abs(percent):
                self.p_agent_is_reached_goal = True
                if best_price < present_price :
                    best_price = present_price
            else:
                pass


        return self._get_observation(), self._rewards(), self._is_done(), [{'stop_loss': self.p_agent_is_stop_loss,
                                                                            'stop_loss_price' :self.p_agent_is_stop_loss_price,
                                                                            'reached_profit': self.p_agent_is_reached_goal,
                                                                            'best_price':best_price}]

    def reset(self):
        """Reset the state of the environment and returns an initial observation.

        here we will reset agent's enviroment to restart.
        for now, we need to have its database to load and the status will go back to its first status like it called init

        everytime, it needs to reset index to reference which index of episode data agent use training data.

        Returns:
            numpy.array: The initial observation of the space. Initial reward is assumed to be 0.
        """
        self.p_agent_current_episode_ref_idx = random.randint(0, self.episode_data_count)
        self.p_agent_current_step_in_episode = 0

        self.p_agent_current_episode_ticker = self.d_episodes_data[self.p_agent_current_episode_ref_idx]['meta']['ticker']
        self.p_agent_current_episode_date = self.d_episodes_data[self.p_agent_current_episode_ref_idx]['meta']['date']
        self.p_agent_current_episode_data_quote = self.d_episodes_data[self.p_agent_current_episode_ref_idx]['quote']
        self.p_agent_current_episode_data_order = self.d_episodes_data[self.p_agent_current_episode_ref_idx]['order']

        current_date = self.p_agent_current_episode_date

        # it can not declared in init method. because we need to consider yyyymmdd in future.
        self.c_agent_prev_step_start_datetime_in_episode = datetime.datetime(int(current_date[0:4]),
                                                                             int(current_date[4:6]),
                                                                             int(current_date[6:8]), 9, 5)
        self.c_agent_step_start_datetime_in_episode = datetime.datetime(int(current_date[0:4]),
                                                                        int(current_date[4:6]),
                                                                        int(current_date[6:8]), 9, 6)

        start = datetime.datetime(int(current_date[0:4]), int(current_date[4:6]), int(current_date[6:8]), 9, 5)
        prev_read_rng = pd.date_range(start, periods=60, freq='S')

        for prev_time_step in prev_read_rng:
            self.p_agent_current_episode_price_history.append(self.p_agent_current_episode_data_quote[prev_time_step]['price'])

        return True

    def render(self):
        """
        Render the environment.
        display agent and gym's status based on a user configures
        """
        logging.info(self._get_status())

    def _get_status(self):
        logs = {}
        logs['agent_current_num_transaction'] = self.p_agent_current_num_transaction
        return logs